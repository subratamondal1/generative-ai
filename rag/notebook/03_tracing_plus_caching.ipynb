{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__aenter__',\n",
       " '__aexit__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_base_url',\n",
       " '_build_headers',\n",
       " '_build_request',\n",
       " '_calculate_retry_timeout',\n",
       " '_client',\n",
       " '_custom_headers',\n",
       " '_custom_query',\n",
       " '_default_stream_cls',\n",
       " '_enforce_trailing_slash',\n",
       " '_idempotency_header',\n",
       " '_idempotency_key',\n",
       " '_limits',\n",
       " '_make_sse_decoder',\n",
       " '_make_status_error',\n",
       " '_make_status_error_from_response',\n",
       " '_maybe_override_cast_to',\n",
       " '_parse_retry_after_header',\n",
       " '_platform',\n",
       " '_prepare_options',\n",
       " '_prepare_request',\n",
       " '_prepare_url',\n",
       " '_process_response',\n",
       " '_process_response_data',\n",
       " '_proxies',\n",
       " '_request',\n",
       " '_request_api_list',\n",
       " '_retry_request',\n",
       " '_serialize_multipartform',\n",
       " '_should_retry',\n",
       " '_should_stream_response_body',\n",
       " '_strict_response_validation',\n",
       " '_transport',\n",
       " '_validate_headers',\n",
       " '_version',\n",
       " 'api_key',\n",
       " 'audio',\n",
       " 'auth_headers',\n",
       " 'base_url',\n",
       " 'batches',\n",
       " 'beta',\n",
       " 'chat',\n",
       " 'close',\n",
       " 'completions',\n",
       " 'copy',\n",
       " 'custom_auth',\n",
       " 'default_headers',\n",
       " 'default_query',\n",
       " 'delete',\n",
       " 'embeddings',\n",
       " 'files',\n",
       " 'fine_tuning',\n",
       " 'get',\n",
       " 'get_api_list',\n",
       " 'images',\n",
       " 'is_closed',\n",
       " 'max_retries',\n",
       " 'models',\n",
       " 'moderations',\n",
       " 'organization',\n",
       " 'patch',\n",
       " 'platform_headers',\n",
       " 'post',\n",
       " 'project',\n",
       " 'put',\n",
       " 'qs',\n",
       " 'request',\n",
       " 'timeout',\n",
       " 'uploads',\n",
       " 'user_agent',\n",
       " 'with_options',\n",
       " 'with_raw_response',\n",
       " 'with_streaming_response']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langfuse.openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "\n",
    "dir(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _msg(role: str, content: str) -> dict[str, str]:\n",
    "    return {\"role\": role, \"content\": content}\n",
    "\n",
    "\n",
    "def _system(content: str) -> dict[str, str]:\n",
    "    return _msg(role=\"system\", content=content)\n",
    "\n",
    "\n",
    "def _user(content: str) -> dict[str, str]:\n",
    "    return _msg(role=\"user\", content=content)\n",
    "\n",
    "\n",
    "def _assistant(content: str) -> dict[str, str]:\n",
    "    return _msg(role=\"assistant\", content=content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = await client.chat.completions.create(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"What is Caching in Generative AI?\")],\n",
    "    max_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Caching in the context of generative AI refers to the practice of storing '\n",
      " 'previously generated responses or outputs to avoid recalculating them when '\n",
      " 'the same input or similar inputs are encountered again. By saving the '\n",
      " 'results of computationally intensive tasks, caching can significantly '\n",
      " 'improve the efficiency of AI systems, reduce latency, and lower '\n",
      " 'computational costs.\\n'\n",
      " '\\n'\n",
      " 'Here are several key points about caching in generative AI:\\n'\n",
      " '\\n'\n",
      " '1. **Performance Improvement**: Generative models, such as language models '\n",
      " '(e.g., GPT-3, GPT')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(object=completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache import Cache\n",
    "\n",
    "# cache = Cache() # Temporary Cache, refreshes every time the notebook is restarted\n",
    "cache = Cache(directory=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.set(key=\"Welcome\", value=\"Subrata Mondal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subrata Mondal'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.get(key=\"Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert Sync to Async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Cache`** is not async by default, we need to make it async by wrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def set_async(key, value, **kwargs):\n",
    "    return await asyncio.to_thread(cache.set, key, value, **kwargs)\n",
    "\n",
    "\n",
    "async def get_async(key, default=None, **kwargs):\n",
    "    return await asyncio.to_thread(cache.get, key, default, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object get_async at 0x10dead380>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_async(key=\"Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subrata Mondal'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_async(key=\"Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alright! Alright!'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_async(key=\"Invalid Key\", default=\"Alright! Alright!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching LLM Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'78a8f834561c67fa660760d41a61dc62'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "md5(string=b\"subrata\").hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8fe21d329868d5b6aef0d2d118a97c62'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "args = [\"1\", \"2\"]\n",
    "kwargs = dict(a=2, b=4)\n",
    "\n",
    "dirty = str(args) + str(kwargs)\n",
    "md5(dirty.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def make_cache_key(key_name, **kwargs):\n",
    "    kwargs_string = json.dumps(kwargs, sort_keys=True)\n",
    "    kwargs_hash = md5(kwargs_string.encode(\"utf-8\")).hexdigest()\n",
    "    cache_key = f\"{key_name}__{kwargs_hash}\"\n",
    "    return cache_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_cache_key_for_chat_completion(model, messages, **kwargs):\n",
    "    return make_cache_key(\n",
    "        key_name=\"openai_chat_completion\",\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletion\n",
    "from functools import update_wrapper\n",
    "\n",
    "CACHE_MISS_SENTINEL = object()\n",
    "\n",
    "\n",
    "async def cached_chat_completion(model, messages, **kwargs) -> ChatCompletion | str:\n",
    "    cache_key = _make_cache_key_for_chat_completion(\n",
    "        model,\n",
    "        messages,\n",
    "        **kwargs,\n",
    "    )\n",
    "    cached_value = await get_async(key=cache_key, default=CACHE_MISS_SENTINEL)\n",
    "\n",
    "    # CACHE MISS\n",
    "    if cached_value is CACHE_MISS_SENTINEL:\n",
    "        completion = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs,\n",
    "        )\n",
    "        await set_async(key=cache_key, value=completion.json())\n",
    "        return completion\n",
    "    # CACHE HIT\n",
    "    else:\n",
    "        return ChatCompletion.validate(json.loads(cached_value))\n",
    "\n",
    "\n",
    "cached_chat_completion = update_wrapper(\n",
    "    wrapper=cached_chat_completion,\n",
    "    wrapped=client.chat.completions.create,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AVdYkVYjYfzq6lS5cRKeNElIjfLpt', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the process of storing copies of files or data in a temporary storage location (the cache) for quick access. The main goal of caching is to improve the performance and efficiency of applications by reducing the time and resources required to fetch data that has already been retrieved or computed.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Storage**:\\n   - Caches can reside in different layers, such as in-memory (RAM), disk, or even in external services (e', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103598, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=100, prompt_tokens=15, total_tokens=115, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.001775979995727539\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "completion = await cached_chat_completion(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"What is Caching in Software Engineering?\")],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AVdYkVYjYfzq6lS5cRKeNElIjfLpt', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the process of storing copies of files or data in a temporary storage location (the cache) for quick access. The main goal of caching is to improve the performance and efficiency of applications by reducing the time and resources required to fetch data that has already been retrieved or computed.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Storage**:\\n   - Caches can reside in different layers, such as in-memory (RAM), disk, or even in external services (e', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103598, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=100, prompt_tokens=15, total_tokens=115, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AVdYkVYjYfzq6lS5cRKeNElIjfLpt', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the process of storing copies of files or data in a temporary storage location (the cache) for quick access. The main goal of caching is to improve the performance and efficiency of applications by reducing the time and resources required to fetch data that has already been retrieved or computed.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Storage**:\\n   - Caches can reside in different layers, such as in-memory (RAM), disk, or even in external services (e', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103598, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=100, prompt_tokens=15, total_tokens=115, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.004051923751831055\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "completion = await cached_chat_completion(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"What is Caching in Software Engineering?\")],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AVdcx8uUPQuCrfxOlOmrT7jxXmQiv', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure! Caching is a technique used in computing to improve the performance of systems by storing copies of frequently accessed data in a location that is faster to access than the original source. Here's a beginner-friendly guide to understanding caching:\\n\\n### What is Caching?\\n\\n1. **Definition**: Caching involves storing data in a temporary storage area (the cache) to speed up future access to that data. Instead of fetching the data from a slow source (like a database or a remote server), the system\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103859, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_3de1288069', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.03742790222167969\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "completion = await cached_chat_completion(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"Teach me Caching like a beginner\")],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AVdcx8uUPQuCrfxOlOmrT7jxXmQiv', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure! Caching is a technique used in computing to improve the performance of systems by storing copies of frequently accessed data in a location that is faster to access than the original source. Here's a beginner-friendly guide to understanding caching:\\n\\n### What is Caching?\\n\\n1. **Definition**: Caching involves storing data in a temporary storage area (the cache) to speed up future access to that data. Instead of fetching the data from a slow source (like a database or a remote server), the system\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103859, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_3de1288069', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.001577138900756836\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "completion = await cached_chat_completion(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"Teach me Caching like a beginner\")],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai-zqkb-BGY-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
