{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _msg(role: str, content: str) -> dict[str, str]:\n",
    "    return {\"role\": role, \"content\": content}\n",
    "\n",
    "\n",
    "def _system(content: str) -> dict[str, str]:\n",
    "    return _msg(role=\"system\", content=content)\n",
    "\n",
    "\n",
    "def _user(content: str) -> dict[str, str]:\n",
    "    return _msg(role=\"user\", content=content)\n",
    "\n",
    "\n",
    "def _assistant(content: str) -> dict[str, str]:\n",
    "    return _msg(role=\"assistant\", content=content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SetUp Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__aenter__',\n",
       " '__aexit__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_base_url',\n",
       " '_build_headers',\n",
       " '_build_request',\n",
       " '_calculate_retry_timeout',\n",
       " '_client',\n",
       " '_custom_headers',\n",
       " '_custom_query',\n",
       " '_default_stream_cls',\n",
       " '_enforce_trailing_slash',\n",
       " '_idempotency_header',\n",
       " '_idempotency_key',\n",
       " '_limits',\n",
       " '_make_sse_decoder',\n",
       " '_make_status_error',\n",
       " '_make_status_error_from_response',\n",
       " '_maybe_override_cast_to',\n",
       " '_parse_retry_after_header',\n",
       " '_platform',\n",
       " '_prepare_options',\n",
       " '_prepare_request',\n",
       " '_prepare_url',\n",
       " '_process_response',\n",
       " '_process_response_data',\n",
       " '_proxies',\n",
       " '_request',\n",
       " '_request_api_list',\n",
       " '_retry_request',\n",
       " '_serialize_multipartform',\n",
       " '_should_retry',\n",
       " '_should_stream_response_body',\n",
       " '_strict_response_validation',\n",
       " '_transport',\n",
       " '_validate_headers',\n",
       " '_version',\n",
       " 'api_key',\n",
       " 'audio',\n",
       " 'auth_headers',\n",
       " 'base_url',\n",
       " 'batches',\n",
       " 'beta',\n",
       " 'chat',\n",
       " 'close',\n",
       " 'completions',\n",
       " 'copy',\n",
       " 'custom_auth',\n",
       " 'default_headers',\n",
       " 'default_query',\n",
       " 'delete',\n",
       " 'embeddings',\n",
       " 'files',\n",
       " 'fine_tuning',\n",
       " 'get',\n",
       " 'get_api_list',\n",
       " 'images',\n",
       " 'is_closed',\n",
       " 'max_retries',\n",
       " 'models',\n",
       " 'moderations',\n",
       " 'organization',\n",
       " 'patch',\n",
       " 'platform_headers',\n",
       " 'post',\n",
       " 'project',\n",
       " 'put',\n",
       " 'qs',\n",
       " 'request',\n",
       " 'timeout',\n",
       " 'uploads',\n",
       " 'user_agent',\n",
       " 'with_options',\n",
       " 'with_raw_response',\n",
       " 'with_streaming_response']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = AsyncOpenAI()\n",
    "GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "\n",
    "dir(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache import Cache\n",
    "\n",
    "# cache = Cache() # Temporary Cache, refreshes every time the notebook is restarted\n",
    "cache = Cache(directory=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.set(key=\"Welcome\", value=\"Subrata Mondal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subrata Mondal'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.get(key=\"Welcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Sync to Async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Cache`** is not async by default, we need to make it async by wrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def set_async(key, value, **kwargs):\n",
    "    return await asyncio.to_thread(cache.set, key, value, **kwargs)\n",
    "\n",
    "\n",
    "async def get_async(key, default=None, **kwargs):\n",
    "    return await asyncio.to_thread(cache.get, key, default, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object get_async at 0x113af4200>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_async(key=\"Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subrata Mondal'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_async(key=\"Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alright! Alright!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_async(key=\"Invalid Key\", default=\"Alright! Alright!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching and Tracing LLM Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'78a8f834561c67fa660760d41a61dc62'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "md5(b\"subrata\").hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8fe21d329868d5b6aef0d2d118a97c62'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "args = [\"1\", \"2\"]\n",
    "kwargs = dict(a=2, b=4)\n",
    "\n",
    "dirty = str(args) + str(kwargs)\n",
    "md5(dirty.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def make_cache_key(key_name, **kwargs):\n",
    "    kwargs_string = json.dumps(kwargs, sort_keys=True)\n",
    "    kwargs_hash = md5(kwargs_string.encode(\"utf-8\")).hexdigest()\n",
    "    cache_key = f\"{key_name}__{kwargs_hash}\"\n",
    "    return cache_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_cache_key_for_chat_completion(model, messages, **kwargs):\n",
    "    return make_cache_key(\n",
    "        key_name=\"openai_chat_completion\",\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletion\n",
    "from functools import update_wrapper, wraps\n",
    "import backoff\n",
    "from openai import APITimeoutError, RateLimitError\n",
    "\n",
    "CACHE_MISS_SENTINEL = object()\n",
    "\n",
    "\n",
    "@wraps(client.chat.completions.create)\n",
    "async def cached_chat_completion(model, messages, **kwargs) -> ChatCompletion | str:\n",
    "    cache_key = _make_cache_key_for_chat_completion(\n",
    "        model,\n",
    "        messages,\n",
    "        **kwargs,\n",
    "    )\n",
    "    cached_value = await get_async(key=cache_key, default=CACHE_MISS_SENTINEL)\n",
    "\n",
    "    # CACHE MISS\n",
    "    if cached_value is CACHE_MISS_SENTINEL:\n",
    "\n",
    "        @backoff.on_exception(\n",
    "            backoff.expo, (APITimeoutError, RateLimitError), max_time=60\n",
    "        )\n",
    "        async def do_call():\n",
    "            return await client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        completion = await do_call()\n",
    "        await set_async(key=cache_key, value=completion.json())\n",
    "        return completion\n",
    "    # CACHE HIT\n",
    "    else:\n",
    "        # TODO: Tracing Code\n",
    "        return ChatCompletion.validate(json.loads(cached_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AVdYkVYjYfzq6lS5cRKeNElIjfLpt', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the process of storing copies of files or data in a temporary storage location (the cache) for quick access. The main goal of caching is to improve the performance and efficiency of applications by reducing the time and resources required to fetch data that has already been retrieved or computed.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Storage**:\\n   - Caches can reside in different layers, such as in-memory (RAM), disk, or even in external services (e', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103598, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=100, prompt_tokens=15, total_tokens=115, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.002405881881713867\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "completion = await cached_chat_completion(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"What is Caching in Software Engineering?\")],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AVdYkVYjYfzq6lS5cRKeNElIjfLpt', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the process of storing copies of files or data in a temporary storage location (the cache) for quick access. The main goal of caching is to improve the performance and efficiency of applications by reducing the time and resources required to fetch data that has already been retrieved or computed.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Storage**:\\n   - Caches can reside in different layers, such as in-memory (RAM), disk, or even in external services (e', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103598, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=100, prompt_tokens=15, total_tokens=115, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AVdYkVYjYfzq6lS5cRKeNElIjfLpt', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the process of storing copies of files or data in a temporary storage location (the cache) for quick access. The main goal of caching is to improve the performance and efficiency of applications by reducing the time and resources required to fetch data that has already been retrieved or computed.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Storage**:\\n   - Caches can reside in different layers, such as in-memory (RAM), disk, or even in external services (e', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103598, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=100, prompt_tokens=15, total_tokens=115, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.0017559528350830078\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "completion = await cached_chat_completion(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"What is Caching in Software Engineering?\")],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AVdcx8uUPQuCrfxOlOmrT7jxXmQiv', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure! Caching is a technique used in computing to improve the performance of systems by storing copies of frequently accessed data in a location that is faster to access than the original source. Here's a beginner-friendly guide to understanding caching:\\n\\n### What is Caching?\\n\\n1. **Definition**: Caching involves storing data in a temporary storage area (the cache) to speed up future access to that data. Instead of fetching the data from a slow source (like a database or a remote server), the system\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103859, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_3de1288069', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.0020401477813720703\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "completion = await cached_chat_completion(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"Teach me Caching like a beginner\")],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AVdcx8uUPQuCrfxOlOmrT7jxXmQiv', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure! Caching is a technique used in computing to improve the performance of systems by storing copies of frequently accessed data in a location that is faster to access than the original source. Here's a beginner-friendly guide to understanding caching:\\n\\n### What is Caching?\\n\\n1. **Definition**: Caching involves storing data in a temporary storage area (the cache) to speed up future access to that data. Instead of fetching the data from a slow source (like a database or a remote server), the system\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103859, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_3de1288069', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.0032231807708740234\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "completion = await cached_chat_completion(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"Teach me Caching like a beginner\")],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai-zqkb-BGY-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
