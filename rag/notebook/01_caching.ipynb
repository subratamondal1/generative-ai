{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "GPT_4O_MINI = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _msg(role: str, content: str) -> dict[str, str]:\n",
    "    return {\"role\": role, \"content\": content}\n",
    "\n",
    "\n",
    "def _system(content: str) -> dict[str, str]:\n",
    "    return _msg(role=\"system\", content=content)\n",
    "\n",
    "\n",
    "def _user(content: str) -> dict[str, str]:\n",
    "    return _msg(role=\"user\", content=content)\n",
    "\n",
    "\n",
    "def _assistant(content: str) -> dict[str, str]:\n",
    "    return _msg(role=\"assistant\", content=content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = await client.chat.completions.create(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"What is Caching in Generative AI?\")],\n",
    "    max_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Caching in generative AI refers to the practice of storing previously '\n",
      " 'generated outputs or intermediate results in order to improve efficiency and '\n",
      " 'reduce computation time for future requests. Hereâ€™s a closer look at how '\n",
      " 'caching can be beneficial in the context of generative AI:\\n'\n",
      " '\\n'\n",
      " '1. **Efficiency Improvement**: By storing the results of computations that '\n",
      " 'are frequently requested, the system can quickly retrieve cached outputs '\n",
      " 'instead of recalculating them. This significantly speeds up response times, '\n",
      " 'especially for popular queries or prompts.\\n'\n",
      " '\\n'\n",
      " '2. **Resource Optimization')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(object=completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache import Cache\n",
    "\n",
    "# cache = Cache() # Temporary Cache, refreshes every time the notebook is restarted\n",
    "cache = Cache(directory=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.set(key=\"Welcome\", value=\"Subrata Mondal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subrata Mondal'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.get(key=\"Welcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Cache`** is not async by default, we need to make it async by wrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def set_async(key, value, **kwargs):\n",
    "    return await asyncio.to_thread(cache.set, key, value, **kwargs)\n",
    "\n",
    "\n",
    "async def get_async(key, default=None, **kwargs):\n",
    "    return await asyncio.to_thread(cache.get, key, default, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object get_async at 0x107ea35a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_async(key=\"Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subrata Mondal'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_async(key=\"Welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alright! Alright!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_async(key=\"Invalid Key\", default=\"Alright! Alright!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching LLM Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'78a8f834561c67fa660760d41a61dc62'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "md5(b\"subrata\").hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8fe21d329868d5b6aef0d2d118a97c62'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "args = [\"1\", \"2\"]\n",
    "kwargs = dict(a=2, b=4)\n",
    "\n",
    "dirty = str(args) + str(kwargs)\n",
    "md5(dirty.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def make_cache_key(key_name, **kwargs):\n",
    "    kwargs_string = json.dumps(kwargs, sort_keys=True)\n",
    "    kwargs_hash = md5(kwargs_string.encode(\"utf-8\")).hexdigest()\n",
    "    cache_key = f\"{key_name}__{kwargs_hash}\"\n",
    "    return cache_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_cache_key_for_chat_completion(model, messages, **kwargs):\n",
    "    return make_cache_key(\n",
    "        key_name=\"openai_chat_completion\",\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletion\n",
    "from functools import update_wrapper\n",
    "\n",
    "CACHE_MISS_SENTINEL = object()\n",
    "\n",
    "\n",
    "async def cached_chat_completion(model, messages, **kwargs) -> ChatCompletion | str:\n",
    "    cache_key = _make_cache_key_for_chat_completion(\n",
    "        model,\n",
    "        messages,\n",
    "        **kwargs,\n",
    "    )\n",
    "    cached_value = await get_async(key=cache_key, default=CACHE_MISS_SENTINEL)\n",
    "\n",
    "    # CACHE MISS\n",
    "    if cached_value is CACHE_MISS_SENTINEL:\n",
    "        completion = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs,\n",
    "        )\n",
    "        await set_async(key=cache_key, value=completion.json())\n",
    "        return completion\n",
    "    # CACHE HIT\n",
    "    else:\n",
    "        return ChatCompletion.validate(json.loads(cached_value))\n",
    "\n",
    "\n",
    "cached_chat_completion = update_wrapper(\n",
    "    wrapper=cached_chat_completion,\n",
    "    wrapped=client.chat.completions.create,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AVdYkVYjYfzq6lS5cRKeNElIjfLpt', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the process of storing copies of files or data in a temporary storage location (the cache) for quick access. The main goal of caching is to improve the performance and efficiency of applications by reducing the time and resources required to fetch data that has already been retrieved or computed.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Storage**:\\n   - Caches can reside in different layers, such as in-memory (RAM), disk, or even in external services (e', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103598, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=100, prompt_tokens=15, total_tokens=115, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.0019919872283935547\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "completion = await cached_chat_completion(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"What is Caching in Software Engineering?\")],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AVdYkVYjYfzq6lS5cRKeNElIjfLpt', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the process of storing copies of files or data in a temporary storage location (the cache) for quick access. The main goal of caching is to improve the performance and efficiency of applications by reducing the time and resources required to fetch data that has already been retrieved or computed.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Storage**:\\n   - Caches can reside in different layers, such as in-memory (RAM), disk, or even in external services (e', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103598, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=100, prompt_tokens=15, total_tokens=115, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AVdYkVYjYfzq6lS5cRKeNElIjfLpt', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Caching in software engineering refers to the process of storing copies of files or data in a temporary storage location (the cache) for quick access. The main goal of caching is to improve the performance and efficiency of applications by reducing the time and resources required to fetch data that has already been retrieved or computed.\\n\\n### Key Concepts of Caching:\\n\\n1. **Cache Storage**:\\n   - Caches can reside in different layers, such as in-memory (RAM), disk, or even in external services (e', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103598, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=100, prompt_tokens=15, total_tokens=115, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.0026187896728515625\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "completion = await cached_chat_completion(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"What is Caching in Software Engineering?\")],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AVdcx8uUPQuCrfxOlOmrT7jxXmQiv', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure! Caching is a technique used in computing to improve the performance of systems by storing copies of frequently accessed data in a location that is faster to access than the original source. Here's a beginner-friendly guide to understanding caching:\\n\\n### What is Caching?\\n\\n1. **Definition**: Caching involves storing data in a temporary storage area (the cache) to speed up future access to that data. Instead of fetching the data from a slow source (like a database or a remote server), the system\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103859, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_3de1288069', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "2.374541759490967\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "completion = await cached_chat_completion(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"Teach me Caching like a beginner\")],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AVdcx8uUPQuCrfxOlOmrT7jxXmQiv', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure! Caching is a technique used in computing to improve the performance of systems by storing copies of frequently accessed data in a location that is faster to access than the original source. Here's a beginner-friendly guide to understanding caching:\\n\\n### What is Caching?\\n\\n1. **Definition**: Caching involves storing data in a temporary storage area (the cache) to speed up future access to that data. Instead of fetching the data from a slow source (like a database or a remote server), the system\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732103859, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_3de1288069', usage=CompletionUsage(completion_tokens=100, prompt_tokens=14, total_tokens=114, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.0035142898559570312\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "completion = await cached_chat_completion(\n",
    "    model=GPT_4O_MINI,\n",
    "    messages=[_user(content=\"Teach me Caching like a beginner\")],\n",
    "    max_tokens=100,\n",
    ")\n",
    "print(completion)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai-zqkb-BGY-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
