{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Generative AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/admin/.pyenv/versions/3.12.5/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/admin/.pyenv/versions/3.12.5/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/admin/.pyenv/versions/3.12.5/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/pp/3zwp4_s56jb51k20dlwb3v8h0000gn/T/ipykernel_11117/312053368.py\", line 15, in <module>\n",
      "    import torch\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/admin/Library/Caches/pypoetry/virtualenvs/generative-ai-zqkb-BGY-py3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "import ipdb\n",
    "import requests\n",
    "\n",
    "# Tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from requests import Response\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files using Python ...\n",
      "Download complete\n"
     ]
    }
   ],
   "source": [
    "# Download necessary files\n",
    "files_url = \"https://ideami.com/llm_train\"\n",
    "print(\"Downloading files using Python ...\")\n",
    "response: Response = requests.get(files_url)\n",
    "\n",
    "with zipfile.ZipFile(file=io.BytesIO(initial_bytes=response.content)) as zip_ref:\n",
    "    zip_ref.extractall(path=\"./data\")\n",
    "print(\"Download complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:cpu\n"
     ]
    }
   ],
   "source": [
    "# Architecture Parameters\n",
    "batch_size = 8\n",
    "context = 512\n",
    "embed_size = 384\n",
    "n_layers = 7\n",
    "n_heads = 7\n",
    "BIAS = True\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "dropout = 0.05\n",
    "weight_decay = 0.01\n",
    "grad_clip = 1.0\n",
    "epochs = 10\n",
    "\n",
    "# Training Parameters\n",
    "train_iters = 100000\n",
    "eval_interval = 50\n",
    "eval_iters = 10\n",
    "compile = True\n",
    "checkpoint_dir = \"models\"\n",
    "checkpoint_file_name = \"latest.pt\"\n",
    "checkpoint_load_file_name = \"latest.pt\"\n",
    "dtype: torch.dtype = torch.bfloat16\n",
    "\n",
    "# Mode\n",
    "inference = False\n",
    "\n",
    "# Device\n",
    "device = torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device:{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/admin/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/admin/Workspace/generative-ai/llm mastery/notebooks/wandb/run-20241018_163256-fp3hapxm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iamsubrata__/llm1/runs/fp3hapxm' target=\"_blank\">lm120241018-163233</a></strong> to <a href='https://wandb.ai/iamsubrata__/llm1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iamsubrata__/llm1' target=\"_blank\">https://wandb.ai/iamsubrata__/llm1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iamsubrata__/llm1/runs/fp3hapxm' target=\"_blank\">https://wandb.ai/iamsubrata__/llm1/runs/fp3hapxm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logging\n",
    "wandb_log = True\n",
    "wandb_project = \"llm1\"\n",
    "wandb_run_name: str = \"lm1\" + datetime.now().strftime(format=\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178,255,102\n",
      "'s treatment \"appalling\".\n",
      "\n",
      "\n",
      "Alanis Morissette\n",
      "\n",
      "Alanis Nadine Morissette (born June 1, 1974) is a Grammy Award-winning Canadian-American singer and songwriter. She was born in Ottawa, Canada. She began singing in Canada as a teenager in 1990. In 1995, she became popular all over the world.\n",
      "\n",
      "As a young child in Canada, Morissette began to act on television, including 5 episodes of the long-running series, \"You Can't Do That on Television\". Her first album was released only in Canada in 1990.\n",
      "\n",
      "Her \n"
     ]
    }
   ],
   "source": [
    "with open(file=\"./data/wiki.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    text: str = f.read()\n",
    "\n",
    "print(f\"{len(text):,}\")\n",
    "print(text[20000:20500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 4096\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "sp = spm.SentencePieceProcessor(model_file=\"./data/wiki_tokenizer.model\")\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Vocab Size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[312, 471, 4037, 870, 36]\n",
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "def encode(s: str) -> list[int]:\n",
    "    return sp.Encode(input=s)\n",
    "\n",
    "\n",
    "def decode(n: list[int]) -> str:\n",
    "    return sp.DecodeIds(input=n)\n",
    "\n",
    "\n",
    "print(encode(s=\"Hello World!\"))\n",
    "print(decode([312, 471, 4037, 870, 36]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoding ...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(path=\"./data/encoded_data.pt\"):\n",
    "    print(\"Loading encoding ...\")\n",
    "    data = torch.load(f=\"./data/encoded_data.pt\")\n",
    "else:\n",
    "    data = torch.tensor(data=encode(s=text), dtype=torch.long)\n",
    "    torch.save(obj=data, f=\"./data/encoded_data.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec: 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai-zqkb-BGY-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
