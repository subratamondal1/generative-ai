{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain Basics"
      ],
      "metadata": {
        "id": "kkr_vbbZEhOK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzuS3qHycLUF",
        "outputId": "c21d4197-87f8-4cd2-adb4-8f557e039d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/203.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/203.2 kB\u001b[0m \u001b[31m726.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/203.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m194.6/203.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCPU times: user 13.2 ms, sys: 8.1 ms, total: 21.3 ms\n",
            "Wall time: 2.81 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "!pip install openai langchain langchain_community langchain_experimental langchain_openai pinecone-client python-dotenv tiktoken -Uq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show openai langchain pinecone-client python-dotenv tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnjU0jvJEumb",
        "outputId": "004f7ec1-060d-49cf-dffb-b96a56677272"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: openai\n",
            "Version: 1.37.1\n",
            "Summary: The official Python library for the openai API\n",
            "Home-page: https://github.com/openai/openai-python\n",
            "Author: \n",
            "Author-email: OpenAI <support@openai.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\n",
            "Required-by: langchain-openai\n",
            "---\n",
            "Name: langchain\n",
            "Version: 0.2.11\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: langchain-community\n",
            "---\n",
            "Name: pinecone-client\n",
            "Version: 5.0.0\n",
            "Summary: Pinecone client and SDK\n",
            "Home-page: https://www.pinecone.io\n",
            "Author: Pinecone Systems, Inc.\n",
            "Author-email: support@pinecone.io\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: certifi, pinecone-plugin-inference, pinecone-plugin-interface, tqdm, typing-extensions, urllib3\n",
            "Required-by: pinecone-plugin-inference\n",
            "---\n",
            "Name: python-dotenv\n",
            "Version: 1.0.1\n",
            "Summary: Read key-value pairs from a .env file and set them as environment variables\n",
            "Home-page: https://github.com/theskumar/python-dotenv\n",
            "Author: Saurabh Kumar\n",
            "Author-email: me+github@saurabh-kumar.com\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "---\n",
            "Name: tiktoken\n",
            "Version: 0.7.0\n",
            "Summary: tiktoken is a fast BPE tokeniser for use with OpenAI's models\n",
            "Home-page: https://github.com/openai/tiktoken\n",
            "Author: Shantanu Jain\n",
            "Author-email: shantanu@openai.com\n",
            "License: MIT License\n",
            "        \n",
            "        Copyright (c) 2022 OpenAI, Shantanu Jain\n",
            "        \n",
            "        Permission is hereby granted, free of charge, to any person obtaining a copy\n",
            "        of this software and associated documentation files (the \"Software\"), to deal\n",
            "        in the Software without restriction, including without limitation the rights\n",
            "        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
            "        copies of the Software, and to permit persons to whom the Software is\n",
            "        furnished to do so, subject to the following conditions:\n",
            "        \n",
            "        The above copyright notice and this permission notice shall be included in all\n",
            "        copies or substantial portions of the Software.\n",
            "        \n",
            "        THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
            "        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
            "        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
            "        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
            "        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
            "        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
            "        SOFTWARE.\n",
            "        \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: regex, requests\n",
            "Required-by: langchain-openai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community -q"
      ],
      "metadata": {
        "id": "xrQzRPW6z_GG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python Dot-Env"
      ],
      "metadata": {
        "id": "rw2NpwYFI80j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vscode\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(dotenv_path=find_dotenv(), override=True)\n",
        "\n",
        "OPENAI_API_KEY=os.getenv(key=\"OPENAI_API_KEY\")\n",
        "PINECONE_API_KEY=os.getenv(key=\"PINECONE_API_KEY\")\n",
        "LANGCHAIN_API_KEY=os.getenv(key=\"LANGCHAIN_API_KEY\")"
      ],
      "metadata": {
        "id": "KX1f8MHIFXiT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# colab\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY=userdata.get('openai_key')\n",
        "PINECONE_API_KEY=userdata.get('PINECONE_API_KEY')\n",
        "LANGCHAIN_API_KEY=userdata.get('LANGCHAIN_API_KEY')"
      ],
      "metadata": {
        "id": "Mt5iTy1GJHGl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple LLM → `gpt-3.5-turbo-instruct`"
      ],
      "metadata": {
        "id": "oelYmXN0-UuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.openai import OpenAI\n",
        "\n",
        "llm=OpenAI(\n",
        "    model_name=\"gpt-3.5-turbo-instruct\",\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    temperature=0.9,\n",
        "    max_tokens=512\n",
        "  )\n",
        "print(llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWMJJi6nPFJ7",
        "outputId": "8de91d7b-ed7c-490e-cf1c-c388e48315e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mOpenAI\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.9, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'n': 1, 'logit_bias': {}, 'max_tokens': 512}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Explain LLM Wrappers and it's use-cases in three sentences.\"\n",
        "output=llm(prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2IcMzhlzEUd",
        "outputId": "a8a595d0-29fd-4836-8021-a36ac54f8531"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. LLM (Local Linear Mapping) Wrappers are machine learning techniques used for dimensionality reduction in high-dimensional data.\n",
            "2. These methods aim to preserve the local structure of the data by finding a low-dimensional representation that maximally preserves the pairwise distances among the data points.\n",
            "3. LLM Wrappers are particularly useful in data visualization, feature selection, and clustering tasks where high-dimensional data can be more easily understood and analyzed in a lower-dimensional space.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_token_length=llm.get_num_tokens(prompt)\n",
        "print(f\"Prmopt token length: {prompt_token_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnjD24D41INe",
        "outputId": "d0f018f8-ba45-43e2-d89b-042900fff9cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prmopt token length: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate multiple prompts\n",
        "prompt1=\"... is the city of joy.\"\n",
        "prompt2=\"Top 10 richest persons are ...\"\n",
        "output=llm.generate([prompt1, prompt2])\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btp9tAaw2qH3",
        "outputId": "22ab5f83-57db-4d8a-bd32-267f1d99d31c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generations=[[Generation(text=\" A thousand different influences contribute to the amazing energy and vibrancy which make it unique. From innumerable palaces and museums to the never-ending festivals Kolkata is a city that is a feast to the eyes. The city has been home to luminaries like Rabindranath Tagore, Satyajit Ray, Mother Teresa, Subhas Chandra Bose and Netaji by name. Kolkata is also home to India's film industry- Bollywood. Its people are enthusiastic, expressive, moody, musical and artistic-by human...\\n\\n754 Words | 2 Pages\\n\\nOpen Document\\n\\nWhy Our Veterans Should Be Honored\\n\\nSo why should we honor our veterans? I think we should honor our veterans for many reasons and many ways. I will... explain my reasons in this essay. The first reason why we should honor veterans is because they fought for our freedom. You may ask, what is freedom? Freedom is the power or right to act, speak, or think as one wants without hindrance or restraint. My grandfather, like many veterans, fought for freedom. He fought for freedom so that we could have the right to act, speak, and think without...\\n\\n848 Words | 3 Pages\\n\\nOpen Document\\n\\nWhy I Am Optimistic About America's Future\\n\\nI am optimistic about America's future because I believe in American people. I believe... that in the times of enormous difficulties and unprecedented challenges Americans unite and overcome any obstacle. Today, when America is facing such enormous challenges as economical crisis, unemployment and a huge federal deficit Americans will find a way to overcome them and enter to better and more prosperous future. I believe in American spirit of freedom, liberty, and courage which allow American people...\\n\\n680 Words | 2 Pages\\n\\nOpen Document\\n\\nWinston Churchill: Why He Is a Hero\\n\\n\\ufeffWinston Churchill: Why He Is a Hero Winston Churchill was born on November 30, 1874, in Blenheim Palace,... Oxfordshire in England. His father was Lord Randolph Churchill, the Duke of Marlborough. His mother was an American, Jennie Jerome. After he graduated from the Royal Military College, Sandhurst, he joined the British Army. He was a writer and he published his first book, The Story of the Malakand Field Force in 1898. Soon after that, he went to South Africa to fight in the Second Boer War where...\\n\\n1311 Words | 4 Pages\\n\\nOpen Document\\n\\nKurt Cobain: Why He Is a Hero\\n\\n\\ufeff April 7, 2014 First Year Composition Kurt\", generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text='\\n\\n1. Jeff Bezos\\n2. Bill Gates\\n3. Warren Buffett\\n4. Bernard Arnault & family\\n5. Amancio Ortega\\n6. Larry Ellison\\n7. Larry Page\\n8. Sergey Brin\\n9. Steve Ballmer\\n10. Mukesh Ambani', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 587, 'completion_tokens': 573, 'prompt_tokens': 14}, 'model_name': 'gpt-3.5-turbo-instruct'} run=[RunInfo(run_id=UUID('31412cd2-88b5-440f-a268-66910cf4bcd4')), RunInfo(run_id=UUID('af524acd-5830-4407-9536-d00ea45ecfc9'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(output.generations), output.generations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJwRuaRU4oNC",
        "outputId": "cbcda5a4-faa5-4bae-bb49-383eee4dd4ba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 [[Generation(text=\" A thousand different influences contribute to the amazing energy and vibrancy which make it unique. From innumerable palaces and museums to the never-ending festivals Kolkata is a city that is a feast to the eyes. The city has been home to luminaries like Rabindranath Tagore, Satyajit Ray, Mother Teresa, Subhas Chandra Bose and Netaji by name. Kolkata is also home to India's film industry- Bollywood. Its people are enthusiastic, expressive, moody, musical and artistic-by human...\\n\\n754 Words | 2 Pages\\n\\nOpen Document\\n\\nWhy Our Veterans Should Be Honored\\n\\nSo why should we honor our veterans? I think we should honor our veterans for many reasons and many ways. I will... explain my reasons in this essay. The first reason why we should honor veterans is because they fought for our freedom. You may ask, what is freedom? Freedom is the power or right to act, speak, or think as one wants without hindrance or restraint. My grandfather, like many veterans, fought for freedom. He fought for freedom so that we could have the right to act, speak, and think without...\\n\\n848 Words | 3 Pages\\n\\nOpen Document\\n\\nWhy I Am Optimistic About America's Future\\n\\nI am optimistic about America's future because I believe in American people. I believe... that in the times of enormous difficulties and unprecedented challenges Americans unite and overcome any obstacle. Today, when America is facing such enormous challenges as economical crisis, unemployment and a huge federal deficit Americans will find a way to overcome them and enter to better and more prosperous future. I believe in American spirit of freedom, liberty, and courage which allow American people...\\n\\n680 Words | 2 Pages\\n\\nOpen Document\\n\\nWinston Churchill: Why He Is a Hero\\n\\n\\ufeffWinston Churchill: Why He Is a Hero Winston Churchill was born on November 30, 1874, in Blenheim Palace,... Oxfordshire in England. His father was Lord Randolph Churchill, the Duke of Marlborough. His mother was an American, Jennie Jerome. After he graduated from the Royal Military College, Sandhurst, he joined the British Army. He was a writer and he published his first book, The Story of the Malakand Field Force in 1898. Soon after that, he went to South Africa to fight in the Second Boer War where...\\n\\n1311 Words | 4 Pages\\n\\nOpen Document\\n\\nKurt Cobain: Why He Is a Hero\\n\\n\\ufeff April 7, 2014 First Year Composition Kurt\", generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text='\\n\\n1. Jeff Bezos\\n2. Bill Gates\\n3. Warren Buffett\\n4. Bernard Arnault & family\\n5. Amancio Ortega\\n6. Larry Ellison\\n7. Larry Page\\n8. Sergey Brin\\n9. Steve Ballmer\\n10. Mukesh Ambani', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.generations[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4NRDJ_i30HS",
        "outputId": "3f8c4ab1-5976-4825-f77a-cd663be34adc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text=\" A thousand different influences contribute to the amazing energy and vibrancy which make it unique. From innumerable palaces and museums to the never-ending festivals Kolkata is a city that is a feast to the eyes. The city has been home to luminaries like Rabindranath Tagore, Satyajit Ray, Mother Teresa, Subhas Chandra Bose and Netaji by name. Kolkata is also home to India's film industry- Bollywood. Its people are enthusiastic, expressive, moody, musical and artistic-by human...\\n\\n754 Words | 2 Pages\\n\\nOpen Document\\n\\nWhy Our Veterans Should Be Honored\\n\\nSo why should we honor our veterans? I think we should honor our veterans for many reasons and many ways. I will... explain my reasons in this essay. The first reason why we should honor veterans is because they fought for our freedom. You may ask, what is freedom? Freedom is the power or right to act, speak, or think as one wants without hindrance or restraint. My grandfather, like many veterans, fought for freedom. He fought for freedom so that we could have the right to act, speak, and think without...\\n\\n848 Words | 3 Pages\\n\\nOpen Document\\n\\nWhy I Am Optimistic About America's Future\\n\\nI am optimistic about America's future because I believe in American people. I believe... that in the times of enormous difficulties and unprecedented challenges Americans unite and overcome any obstacle. Today, when America is facing such enormous challenges as economical crisis, unemployment and a huge federal deficit Americans will find a way to overcome them and enter to better and more prosperous future. I believe in American spirit of freedom, liberty, and courage which allow American people...\\n\\n680 Words | 2 Pages\\n\\nOpen Document\\n\\nWinston Churchill: Why He Is a Hero\\n\\n\\ufeffWinston Churchill: Why He Is a Hero Winston Churchill was born on November 30, 1874, in Blenheim Palace,... Oxfordshire in England. His father was Lord Randolph Churchill, the Duke of Marlborough. His mother was an American, Jennie Jerome. After he graduated from the Royal Military College, Sandhurst, he joined the British Army. He was a writer and he published his first book, The Story of the Malakand Field Force in 1898. Soon after that, he went to South Africa to fight in the Second Boer War where...\\n\\n1311 Words | 4 Pages\\n\\nOpen Document\\n\\nKurt Cobain: Why He Is a Hero\\n\\n\\ufeff April 7, 2014 First Year Composition Kurt\" generation_info={'finish_reason': 'length', 'logprobs': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# access only the first prompt result\n",
        "print(output.generations[0][0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F1_gCu84Qdr",
        "outputId": "c6007157-d5a3-496c-fba4-118de2ba2e83"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A thousand different influences contribute to the amazing energy and vibrancy which make it unique. From innumerable palaces and museums to the never-ending festivals Kolkata is a city that is a feast to the eyes. The city has been home to luminaries like Rabindranath Tagore, Satyajit Ray, Mother Teresa, Subhas Chandra Bose and Netaji by name. Kolkata is also home to India's film industry- Bollywood. Its people are enthusiastic, expressive, moody, musical and artistic-by human...\n",
            "\n",
            "754 Words | 2 Pages\n",
            "\n",
            "Open Document\n",
            "\n",
            "Why Our Veterans Should Be Honored\n",
            "\n",
            "So why should we honor our veterans? I think we should honor our veterans for many reasons and many ways. I will... explain my reasons in this essay. The first reason why we should honor veterans is because they fought for our freedom. You may ask, what is freedom? Freedom is the power or right to act, speak, or think as one wants without hindrance or restraint. My grandfather, like many veterans, fought for freedom. He fought for freedom so that we could have the right to act, speak, and think without...\n",
            "\n",
            "848 Words | 3 Pages\n",
            "\n",
            "Open Document\n",
            "\n",
            "Why I Am Optimistic About America's Future\n",
            "\n",
            "I am optimistic about America's future because I believe in American people. I believe... that in the times of enormous difficulties and unprecedented challenges Americans unite and overcome any obstacle. Today, when America is facing such enormous challenges as economical crisis, unemployment and a huge federal deficit Americans will find a way to overcome them and enter to better and more prosperous future. I believe in American spirit of freedom, liberty, and courage which allow American people...\n",
            "\n",
            "680 Words | 2 Pages\n",
            "\n",
            "Open Document\n",
            "\n",
            "Winston Churchill: Why He Is a Hero\n",
            "\n",
            "﻿Winston Churchill: Why He Is a Hero Winston Churchill was born on November 30, 1874, in Blenheim Palace,... Oxfordshire in England. His father was Lord Randolph Churchill, the Duke of Marlborough. His mother was an American, Jennie Jerome. After he graduated from the Royal Military College, Sandhurst, he joined the British Army. He was a writer and he published his first book, The Story of the Malakand Field Force in 1898. Soon after that, he went to South Africa to fight in the Second Boer War where...\n",
            "\n",
            "1311 Words | 4 Pages\n",
            "\n",
            "Open Document\n",
            "\n",
            "Kurt Cobain: Why He Is a Hero\n",
            "\n",
            "﻿ April 7, 2014 First Year Composition Kurt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Multiple Responses for Single Prompt"
      ],
      "metadata": {
        "id": "kyZFecgq7wXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# asked llm to generate multiple responses\n",
        "prompt=\"Write tagline for a software company that does outsourcing.\"\n",
        "outputs=llm.generate([prompt], n=2)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX9WmCYv4fJl",
        "outputId": "17b57701-f385-4aec-a78b-bb2feacd4767"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generations=[[Generation(text='\\n\"Empowering your business, one outsourced solution at a time.\"', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\"Empowering your business through custom solutions – Let us handle the rest.\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 42, 'completion_tokens': 31, 'prompt_tokens': 11}, 'model_name': 'gpt-3.5-turbo-instruct'} run=[RunInfo(run_id=UUID('064c4051-4c8a-4fe3-83b5-d4e973e826c1'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(outputs.generations[0]), outputs.generations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8KGOPeN5moc",
        "outputId": "8875c48b-7826-491f-f2ef-2bbdd572c83a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 [[Generation(text='\\n\"Empowering your business, one outsourced solution at a time.\"', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text='\\n\"Empowering your business through custom solutions – Let us handle the rest.\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index,output in enumerate(outputs.generations[0]):\n",
        "  print(output.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JTP0Ddp5q5c",
        "outputId": "6c0209c2-0e27-49e1-ad2c-1c723ee6d1bf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\"Empowering your business, one outsourced solution at a time.\"\n",
            "\n",
            "\"Empowering your business through custom solutions – Let us handle the rest.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Multiple Responses from Multiple Prompts"
      ],
      "metadata": {
        "id": "BpwTZmO_76nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# asked llm to generate multiple responses\n",
        "prompt1=\"Write tagline for an AI Company.\"\n",
        "prompt2=\"Write a tagline for a Sweet Company.\"\n",
        "outputs=llm.generate([prompt1,prompt2] * 4)\n",
        "\n",
        "print(outputs)\n",
        "print(len(outputs.generations[0]), outputs.generations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0xG3wuP6GNE",
        "outputId": "c06d5ea3-9d2d-4890-80dd-e21a3f5f7460"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generations=[[Generation(text='\\n\\n\"Empowering the Future with Intelligent Solutions: Revolutionizing the World with AI.\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Satisfy your cravings with our irresistible treats!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Unlocking the Future with Cutting-Edge AI Solutions.\" ', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Indulge in sweetness, one delectable treat at a time.\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Empowering the future with intelligent technology.\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Satisfy your sweet tooth with every bite!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Revolutionizing the Future with Intelligent Solutions: Empowering You with AI\" \\n', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Satisfy your sweet tooth with every bite!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'total_tokens': 177, 'completion_tokens': 109, 'prompt_tokens': 68}, 'model_name': 'gpt-3.5-turbo-instruct'} run=[RunInfo(run_id=UUID('d3a47dff-2888-44c4-8b31-9d2cbfdfe4e0')), RunInfo(run_id=UUID('f408d3ab-af63-460b-a4dd-fa5730993985')), RunInfo(run_id=UUID('1a4652cc-b7b8-41f4-aa91-5e8eb337d19e')), RunInfo(run_id=UUID('33be0cb8-1195-4321-8a74-09faded2e1f8')), RunInfo(run_id=UUID('c6926d7f-0ab6-4595-8bc6-8aa26d1f5666')), RunInfo(run_id=UUID('059feee3-c1fd-437d-bc57-7a44f5862eeb')), RunInfo(run_id=UUID('cffc3d11-5a14-4dc7-8848-7fb963338320')), RunInfo(run_id=UUID('2f56ba5e-c8f9-47f9-b94c-8262d682978d'))]\n",
            "1 [[Generation(text='\\n\\n\"Empowering the Future with Intelligent Solutions: Revolutionizing the World with AI.\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Satisfy your cravings with our irresistible treats!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Unlocking the Future with Cutting-Edge AI Solutions.\" ', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Indulge in sweetness, one delectable treat at a time.\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Empowering the future with intelligent technology.\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Satisfy your sweet tooth with every bite!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Revolutionizing the Future with Intelligent Solutions: Empowering You with AI\" \\n', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Satisfy your sweet tooth with every bite!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.generations[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTu6iLSk8nsi",
        "outputId": "9f690d67-8e8a-4304-d3ca-6300cafbfcab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Generation(text='\\n\\n\"Empowering the Future with Intelligent Solutions: Revolutionizing the World with AI.\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index,output in enumerate(outputs.generations):\n",
        "  print(output[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgtz3n5R8Kip",
        "outputId": "b38ee25e-9904-4923-83d5-f70154cccc1d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Empowering the Future with Intelligent Solutions: Revolutionizing the World with AI.\"\n",
            "\n",
            "\n",
            "\"Satisfy your cravings with our irresistible treats!\"\n",
            "\n",
            "\n",
            "\"Unlocking the Future with Cutting-Edge AI Solutions.\" \n",
            "\n",
            "\n",
            "\"Indulge in sweetness, one delectable treat at a time.\"\n",
            "\n",
            "\n",
            "\"Empowering the future with intelligent technology.\"\n",
            "\n",
            "\n",
            "\"Satisfy your sweet tooth with every bite!\"\n",
            "\n",
            "\n",
            "\"Revolutionizing the Future with Intelligent Solutions: Empowering You with AI\" \n",
            "\n",
            "\n",
            "\n",
            "\"Satisfy your sweet tooth with every bite!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Models → `gpt-3.5-turbo, gpt-4o, gpt-4o mini`"
      ],
      "metadata": {
        "id": "JIyJoBLE-YjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`AIMessage:`** AIMessage is returned from a chat model as a response to a prompt. This message represents the output of the model and consists of both\n",
        "the raw output as returned by the model together standardized fields\n",
        "(e.g., tool calls, usage metadata) added by the LangChain framework.\n",
        "\n",
        "* **`HumanMessage:`** HumanMessages are messages that are passed in from a human to the model.\n",
        "\n",
        "* **`SystemMessage:`** Message for priming AI behavior. The system message is usually passed in as the first of a sequence of input messages.\n",
        "\n",
        "  ```\n",
        "  messages = [\n",
        "              SystemMessage(\n",
        "                  content=\"You are a helpful assistant! Your name is Bob.\"\n",
        "              ),\n",
        "              HumanMessage(\n",
        "                  content=\"What is your name?\"\n",
        "              )\n",
        "          ]\n",
        "  ```"
      ],
      "metadata": {
        "id": "3oiM42hHPVt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "98JutlqY8vQr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "chat=ChatOpenAI(\n",
        "    model_name=\"gpt-4o\",\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    temperature=0,\n",
        "  )\n",
        "messages=[\n",
        "          SystemMessage(\n",
        "            content=\"You are an expert AI/LLM Engineer who teaches using the Richard Feynman Technique.\"\n",
        "          ),\n",
        "          HumanMessage(\n",
        "            content=\"Explain the entire workflow of developing RAG AI Applications.\"\n",
        "          )\n",
        "        ]\n",
        "print(messages)\n",
        "output=chat(messages)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf5KR76HPG5A",
        "outputId": "af0e9c2b-fd0f-49d0-8a17-fd5076592f66"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You are an expert AI/LLM Engineer who teaches using the Richard Feynman Technique.'), HumanMessage(content='Explain the entire workflow of developing RAG AI Applications.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"Sure! Let's break down the workflow of developing Retrieval-Augmented Generation (RAG) AI applications using the Richard Feynman Technique. This technique involves explaining concepts in simple terms, identifying gaps in understanding, and refining the explanation. Here’s a step-by-step guide:\\n\\n### Step 1: Understand the Basics\\n\\n**Retrieval-Augmented Generation (RAG)** is a hybrid approach that combines retrieval-based and generation-based methods in natural language processing (NLP). The goal is to improve the quality and relevance of generated text by incorporating external knowledge.\\n\\n### Step 2: Break Down the Workflow\\n\\n1. **Problem Definition and Data Collection**\\n   - **Define the Problem**: Clearly understand the problem you are trying to solve. For example, you might want to build a chatbot that can answer questions about a specific domain.\\n   - **Collect Data**: Gather a dataset that includes both the questions and the relevant documents or knowledge base that the model will use to retrieve information.\\n\\n2. **Preprocessing and Data Preparation**\\n   - **Clean the Data**: Remove any irrelevant information, handle missing values, and normalize the text (e.g., lowercasing, removing punctuation).\\n   - **Tokenization**: Split the text into tokens (words or subwords) that the model can understand.\\n   - **Indexing**: Create an index of the documents or knowledge base to enable efficient retrieval. This can be done using tools like Elasticsearch or FAISS.\\n\\n3. **Model Selection and Training**\\n   - **Choose a Retrieval Model**: Select a model for retrieving relevant documents. Common choices include BM25, TF-IDF, or neural retrievers like DPR (Dense Passage Retrieval).\\n   - **Choose a Generation Model**: Select a model for generating text. Popular choices include GPT-3, T5, or BART.\\n   - **Train the Models**: Train the retrieval model on the dataset to learn how to fetch relevant documents. Train the generation model to generate coherent and contextually appropriate responses.\\n\\n4. **Integration of Retrieval and Generation**\\n   - **Pipeline Creation**: Create a pipeline that first retrieves relevant documents based on the input query and then uses the generation model to produce the final response.\\n   - **Contextual Embedding**: Use the retrieved documents to provide context to the generation model. This can be done by concatenating the query with the retrieved documents.\\n\\n5. **Evaluation and Fine-Tuning**\\n   - **Evaluate the Model**: Use metrics like BLEU, ROUGE, or human evaluation to assess the quality of the generated responses.\\n   - **Fine-Tune**: Based on the evaluation, fine-tune the models to improve performance. This may involve adjusting hyperparameters, retraining with more data, or using techniques like reinforcement learning.\\n\\n6. **Deployment and Monitoring**\\n   - **Deploy the Model**: Once satisfied with the performance, deploy the model to a production environment. This could be a web service, mobile app, or any other platform.\\n   - **Monitor Performance**: Continuously monitor the model’s performance in the real world. Collect feedback and make necessary adjustments to maintain and improve the quality of responses.\\n\\n### Step 3: Simplify and Refine\\n\\nTo ensure you understand the workflow, try explaining it in even simpler terms:\\n\\n1. **Identify the problem** and gather relevant data.\\n2. **Prepare the data** by cleaning and organizing it.\\n3. **Choose and train models** for retrieving information and generating text.\\n4. **Combine the models** to create a system that retrieves relevant info and generates responses.\\n5. **Evaluate and improve** the system based on performance metrics.\\n6. **Deploy and monitor** the system to ensure it works well in real-world scenarios.\\n\\n### Step 4: Identify Gaps and Refine\\n\\nIf there are any parts you’re unsure about, revisit those sections, do more research, and refine your explanation. For example, if you’re unclear about how indexing works, you might need to study more about indexing techniques and tools.\\n\\n### Step 5: Teach Back\\n\\nTry teaching this workflow to someone else or even to yourself out loud. If you can explain it clearly and answer questions about it, you’ve mastered the concept!\\n\\nBy following these steps, you can develop a solid understanding of the workflow for developing RAG AI applications and be able to explain it effectively to others.\" response_metadata={'token_usage': {'completion_tokens': 888, 'prompt_tokens': 41, 'total_tokens': 929}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_4e2b2da518', 'finish_reason': 'stop', 'logprobs': None} id='run-14353755-bbfd-429b-8416-2db55b2cefe0-0'\n",
            "CPU times: user 197 ms, sys: 13.3 ms, total: 211 ms\n",
            "Wall time: 14.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQbOhXKrRGZm",
        "outputId": "e8a667aa-5884-4cb8-d9e1-a04dd328bb08"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Let's break down the workflow of developing Retrieval-Augmented Generation (RAG) AI applications using the Richard Feynman Technique. This technique involves explaining concepts in simple terms, identifying gaps in understanding, and refining the explanation. Here’s a step-by-step guide:\n",
            "\n",
            "### Step 1: Understand the Basics\n",
            "\n",
            "**Retrieval-Augmented Generation (RAG)** is a hybrid approach that combines retrieval-based and generation-based methods in natural language processing (NLP). The goal is to improve the quality and relevance of generated text by incorporating external knowledge.\n",
            "\n",
            "### Step 2: Break Down the Workflow\n",
            "\n",
            "1. **Problem Definition and Data Collection**\n",
            "   - **Define the Problem**: Clearly understand the problem you are trying to solve. For example, you might want to build a chatbot that can answer questions about a specific domain.\n",
            "   - **Collect Data**: Gather a dataset that includes both the questions and the relevant documents or knowledge base that the model will use to retrieve information.\n",
            "\n",
            "2. **Preprocessing and Data Preparation**\n",
            "   - **Clean the Data**: Remove any irrelevant information, handle missing values, and normalize the text (e.g., lowercasing, removing punctuation).\n",
            "   - **Tokenization**: Split the text into tokens (words or subwords) that the model can understand.\n",
            "   - **Indexing**: Create an index of the documents or knowledge base to enable efficient retrieval. This can be done using tools like Elasticsearch or FAISS.\n",
            "\n",
            "3. **Model Selection and Training**\n",
            "   - **Choose a Retrieval Model**: Select a model for retrieving relevant documents. Common choices include BM25, TF-IDF, or neural retrievers like DPR (Dense Passage Retrieval).\n",
            "   - **Choose a Generation Model**: Select a model for generating text. Popular choices include GPT-3, T5, or BART.\n",
            "   - **Train the Models**: Train the retrieval model on the dataset to learn how to fetch relevant documents. Train the generation model to generate coherent and contextually appropriate responses.\n",
            "\n",
            "4. **Integration of Retrieval and Generation**\n",
            "   - **Pipeline Creation**: Create a pipeline that first retrieves relevant documents based on the input query and then uses the generation model to produce the final response.\n",
            "   - **Contextual Embedding**: Use the retrieved documents to provide context to the generation model. This can be done by concatenating the query with the retrieved documents.\n",
            "\n",
            "5. **Evaluation and Fine-Tuning**\n",
            "   - **Evaluate the Model**: Use metrics like BLEU, ROUGE, or human evaluation to assess the quality of the generated responses.\n",
            "   - **Fine-Tune**: Based on the evaluation, fine-tune the models to improve performance. This may involve adjusting hyperparameters, retraining with more data, or using techniques like reinforcement learning.\n",
            "\n",
            "6. **Deployment and Monitoring**\n",
            "   - **Deploy the Model**: Once satisfied with the performance, deploy the model to a production environment. This could be a web service, mobile app, or any other platform.\n",
            "   - **Monitor Performance**: Continuously monitor the model’s performance in the real world. Collect feedback and make necessary adjustments to maintain and improve the quality of responses.\n",
            "\n",
            "### Step 3: Simplify and Refine\n",
            "\n",
            "To ensure you understand the workflow, try explaining it in even simpler terms:\n",
            "\n",
            "1. **Identify the problem** and gather relevant data.\n",
            "2. **Prepare the data** by cleaning and organizing it.\n",
            "3. **Choose and train models** for retrieving information and generating text.\n",
            "4. **Combine the models** to create a system that retrieves relevant info and generates responses.\n",
            "5. **Evaluate and improve** the system based on performance metrics.\n",
            "6. **Deploy and monitor** the system to ensure it works well in real-world scenarios.\n",
            "\n",
            "### Step 4: Identify Gaps and Refine\n",
            "\n",
            "If there are any parts you’re unsure about, revisit those sections, do more research, and refine your explanation. For example, if you’re unclear about how indexing works, you might need to study more about indexing techniques and tools.\n",
            "\n",
            "### Step 5: Teach Back\n",
            "\n",
            "Try teaching this workflow to someone else or even to yourself out loud. If you can explain it clearly and answer questions about it, you’ve mastered the concept!\n",
            "\n",
            "By following these steps, you can develop a solid understanding of the workflow for developing RAG AI applications and be able to explain it effectively to others.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PromptTemplate"
      ],
      "metadata": {
        "id": "CeQVieiQYR9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain_openai import OpenAI"
      ],
      "metadata": {
        "id": "fRQN4pohSLmn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "template=\"\"\"You are an expert {designation} who teaches using the Richard Feynman Technique. Keep it under {word_count} words.\"\"\"\n",
        "\n",
        "prompt=PromptTemplate(\n",
        "    input_variables=[\"designation\", \"word_count\"],\n",
        "    template=template\n",
        ")\n",
        "print(prompt)\n",
        "\n",
        "llm=OpenAI(\n",
        "    model_name=\"gpt-3.5-turbo-instruct\",\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    temperature=0.9\n",
        "  )\n",
        "print(llm)\n",
        "\n",
        "output = llm.invoke(prompt.format(designation=\"AI Engineer\", word_count=100))\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjIy_26rqKzE",
        "outputId": "44cb7d4d-fbb0-4722-f58e-303f91b117de"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['designation', 'word_count'] template='You are an expert {designation} who teaches using the Richard Feynman Technique. Keep it under {word_count} words.'\n",
            "\u001b[1mOpenAI\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.9, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'seed': None, 'logprobs': None, 'max_tokens': 256}\n",
            "\n",
            "\n",
            "I am an experienced AI Engineer who uses the Richard Feynman Technique to teach complex concepts in a simple and understandable manner. This method involves breaking down complicated ideas into smaller, more manageable pieces, and explaining them using everyday language. By doing so, I ensure that my students not only understand the material, but also retain the knowledge for a longer period of time. Through this approach, I am able to bridge the gap between abstract theories and practical applications, making learning AI more accessible and engaging for my students. \n",
            "CPU times: user 125 ms, sys: 748 µs, total: 125 ms\n",
            "Wall time: 1.79 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains"
      ],
      "metadata": {
        "id": "WC1nq1VE863a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.llm import LLMChain"
      ],
      "metadata": {
        "id": "x4GUuvyrkRNU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template=\"\"\"You are an expert {designation} who teaches using the Richard Feynman Technique. {question}. Keep it under {word_count} words.\"\"\"\n",
        "prompt=PromptTemplate(\n",
        "    input_variables=[\"designation\", \"question\", \"word_count\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# Create a runnable sequence\n",
        "chain = prompt | llm\n",
        "print(\"Chain:\\n\",chain)\n",
        "\n",
        "output=chain.invoke(input={\n",
        "    \"designation\":\"AI Engineer specialized in advanced Retrieval Augmented Generation (RAG) with Large Language Models.\",\n",
        "    \"question\":\"Teach about the advanced methods of RAG\",\n",
        "    \"word_count\":500\n",
        "})\n",
        "print(\"Output:\\n\",output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuTBY-_BpAgX",
        "outputId": "c3ff1066-c39a-42eb-9ac8-1ee2ebc2773c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain:\n",
            " first=PromptTemplate(input_variables=['designation', 'question', 'word_count'], template='You are an expert {designation} who teaches using the Richard Feynman Technique. {question}. Keep it under {word_count} words.') last=OpenAI(client=<openai.resources.completions.Completions object at 0x7b9212fcde10>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7b92130baaa0>, temperature=0.9, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
            "Output:\n",
            " \n",
            "\n",
            "Retrieval Augmented Generation (RAG) is a cutting-edge technique in the field of natural language processing (NLP), specifically in the realm of large language models. This method combines the power of large pre-trained language models with the ability to retrieve relevant information from a knowledge base, resulting in more accurate and comprehensive language generation.\n",
            "\n",
            "At its core, RAG is a two-stage process. The first stage involves retrieving relevant information from a knowledge base using a retrieval model such as BM25 or TF-IDF. This knowledge base can be any structured or unstructured data source, such as a Wikipedia article or a collection of scientific papers. This retrieval process is similar to how a search engine retrieves information from a database based on a query. However, in RAG, the retrieved information is used as a context for the language generation process.\n",
            "\n",
            "The second stage of RAG involves generating text using a large pre-trained language model, such as GPT-3 or BERT. This model takes in the retrieved information as input and generates text that is consistent with both the retrieved context and the prompt given to the model. This results in more accurate and relevant language generation compared to traditional language models, which generate text without context.\n",
            "\n",
            "One of the key benefits of RAG is its ability\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Chains\n",
        "Sequential Chains are Complex Chains."
      ],
      "metadata": {
        "id": "XcYfm06jFr5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI,ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "ZdHFTz7qAsY3"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output parser\n",
        "struct_parser=StrOutputParser()\n",
        "print(struct_parser)\n",
        "\n",
        "llm1=ChatOpenAI(\n",
        "    name=\"gpt-3.5-turbo\",\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    temperature=0.9,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "prompt1=PromptTemplate(\n",
        "    input_variables=[\"input\"],\n",
        "    template=\"\"\"Solve the math expression: {input}\"\"\"\n",
        ")\n",
        "\n",
        "chain1=prompt1|llm1|struct_parser\n",
        "print(chain1)\n",
        "\n",
        "llm2=ChatOpenAI(\n",
        "    name=\"gpt-4o\",\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    temperature=0.2,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "prompt2=PromptTemplate(\n",
        "    input_variables=[\"input\"],\n",
        "    template=\"\"\"Solve the math expression: {input}\"\"\"\n",
        ")\n",
        "\n",
        "chain2=prompt2|llm2|struct_parser\n",
        "print(chain2)\n",
        "\n",
        "sequential_chain=chain1|chain2\n",
        "\n",
        "output=sequential_chain.invoke(input={\n",
        "    \"input\": \"5.1**7.3\"\n",
        "})\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UP006qHFZLU",
        "outputId": "6b7bdc04-d4f2-4d7c-ba50-1e124f7cfda5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "first=PromptTemplate(input_variables=['input'], template='Solve the math expression: {input}') middle=[ChatOpenAI(name='gpt-3.5-turbo', client=<openai.resources.chat.completions.Completions object at 0x7b92103d3730>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7b92103f3a00>, temperature=0.9, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)] last=StrOutputParser()\n",
            "first=PromptTemplate(input_variables=['input'], template='Solve the math expression: {input}') middle=[ChatOpenAI(name='gpt-4o', client=<openai.resources.chat.completions.Completions object at 0x7b92103bda50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7b921039c670>, temperature=0.2, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1024)] last=StrOutputParser()\n",
            "4074.34193072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fZwLQyqYVP96",
        "outputId": "43189721-8fd8-447e-bac9-5b87b7098f61"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4074.34193072'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "Lq8gDKzJfMK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
        "from langchain_experimental.tools import PythonREPLTool\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "cc56ONV3e7pX"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    temperature=0.9,\n",
        "    max_tokens=1024\n",
        ")\n",
        "agent_executor=create_python_agent(\n",
        "    llm=llm,\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "agent_executor.invoke(input=\"Calculate the square root of the factorial of 20 and display it with 4 decimal places.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5Z-0MKDp3pO",
        "outputId": "c9a4d548-94b4-4f72-bcc3-aaf917702b72"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mTo solve this problem, I need to calculate the factorial of 20 and then find the square root of that value. Finally, I will format the result to 4 decimal places.\n",
            "\n",
            "Action: Python_REPL\n",
            "Action Input: \n",
            "```python\n",
            "import math\n",
            "\n",
            "# Calculate the factorial of 20\n",
            "factorial_20 = math.factorial(20)\n",
            "\n",
            "# Calculate the square root of the factorial\n",
            "sqrt_factorial_20 = math.sqrt(factorial_20)\n",
            "\n",
            "# Print the result with 4 decimal places\n",
            "formatted_result = f\"{sqrt_factorial_20:.4f}\"\n",
            "print(formatted_result)\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m1559776268.6285\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
            "\n",
            "Final Answer: 1559776268.6285\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Calculate the square root of the factorial of 20 and display it with 4 decimal places.',\n",
              " 'output': '1559776268.6285'}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinecone Vector DB\n",
        "Diving into Vector Databases."
      ],
      "metadata": {
        "id": "XWlKfioE6qIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY=userdata.get(key=\"PINECONE_API_KEY\")"
      ],
      "metadata": {
        "id": "SyS_xxyOwmW7"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec"
      ],
      "metadata": {
        "id": "GlzIjjrH7Nak"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone=Pinecone(\n",
        "    api_key=PINECONE_API_KEY\n",
        "  )"
      ],
      "metadata": {
        "id": "J_J566DC7Xhy"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pinecone Indexes"
      ],
      "metadata": {
        "id": "W-FCbLyx8YtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.list_indexes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8sIR1DE7hBQ",
        "outputId": "6801257e-1523-478d-bf7b-0d7e9995b7f8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'indexes': [{'deletion_protection': 'disabled',\n",
              "              'dimension': 512,\n",
              "              'host': 'flowise-0390e5b.svc.gcp-starter.pinecone.io',\n",
              "              'metric': 'cosine',\n",
              "              'name': 'flowise',\n",
              "              'spec': {'pod': {'environment': 'gcp-starter',\n",
              "                               'pod_type': 'starter',\n",
              "                               'pods': 1,\n",
              "                               'replicas': 1,\n",
              "                               'shards': 1}},\n",
              "              'status': {'ready': True, 'state': 'Ready'}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9qFL82e-8Ewn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}