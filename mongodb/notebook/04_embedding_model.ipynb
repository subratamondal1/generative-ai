{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Embedding Model\n",
    "> **Embeddings** models are specialized algorithms that reduces High Dimensional Data (such as text, images, audio, video) into a Low Dimensional Space of Dense Vectors.\n",
    "\n",
    "> **LLMs** are effective Artificial Neural Networks Pre-Trained on gigantic corpus of Textual Data.\n",
    "\n",
    "> While both the Embeddings and LLMs are rooted to Neural Networks, they employ distinct methodologies. LLMs are designed for generating coherent and contextually relevant text, while Embeddings are focused on mapping words, phrases, sentences into dense vectors for capturing semantic relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Points to remember:**\n",
    "1. **Embedding Models:**  \n",
    "   These models map words, phrases, or entire sentences into dense vector spaces where semantic relationships are maintained. This means that similar meanings are clustered (positioned close) together in this space.\n",
    "\n",
    "2. **Contrastive Loss:**  \n",
    "   A technique used in embedding models to differentiate between similar and dissimilar pairs during training. It helps the model learn which items should be close in the embedding space and which should be far apart.\n",
    "\n",
    "3. **Positive and Negative Sampling:**  \n",
    "   - **Positive Samples (Minimizes the distance between Positive Pairs):** These are similar items, like synonyms or related sentences. They help the model learn what should be grouped together.  \n",
    "   - **Negative Samples (Maximizes the distance between Negative Pairs):** These are dissimilar items, like unrelated words or sentences, used to teach the model what should be kept apart.\n",
    "\n",
    "<img src=\"./assets/contrastive_loss.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **When to Use Embedding Models**\n",
    "Embedding models are best for tasks that involve understanding and leveraging semantic relationships within data. They are ideal for:\n",
    "- **Semantic Similarity:** Finding or recommending similar items (documents, products) based on content.  \n",
    "- **Clustering:** Grouping entities with similar semantic properties.  \n",
    "- **Information Retrieval:** Enhancing search capabilities by understanding query meanings.\n",
    "\n",
    "### **When to Use Large Language Models (LLMs)**\n",
    "LLMs are suited for tasks that require deep understanding, text generation, or both. They excel in:\n",
    "- **Content Creation:** Generating coherent and stylistically correct text (e.g., writing a movie synopsis).  \n",
    "- **Conversational AI:** Building chatbots and virtual assistants capable of natural, human-like conversations.  \n",
    "- **Language Translation:** Handling complex translations with cultural and linguistic nuances.\n",
    "\n",
    "### **Key Takeaway**\n",
    "- **Embedding Models:** Focus on compact, semantic representation for search, recommendation, and clustering.  \n",
    "- **LLMs:** Excel in generating, interpreting, and understanding complex text.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Key Differences Across Embedding Types**\n",
    "\n",
    "| **Embedding Type**               | **Purpose**                                      | **Strengths**                                   | **Examples**                       |\n",
    "|----------------------------------|--------------------------------------------------|-------------------------------------------------|-----------------------------------|\n",
    "| **Word Embeddings**              | Represent individual words.                      | Captures basic semantic similarity.             | Word2Vec, GloVe, fastText         |\n",
    "| **Sentence/Document Embeddings** | Represent longer texts.                          | Captures overall meaning of sentences/docs.     | Doc2Vec, BERT                    |\n",
    "| **Contextual Embeddings**        | Dynamic word meaning based on context.           | Context-aware, handles polysemy (multiple meanings). | BERT, ELMo, GPT                |\n",
    "| **Specialized Embeddings**       | Focus on specific linguistic properties.         | Handles rare words, dialects, and complex languages. | fastText                       |\n",
    "| **Image Embeddings**             | Represent visual data as vectors.                | Captures image features (edges, textures, patterns). | VGG, ResNet                    |\n",
    "| **Audio Embeddings**             | Represent audio signals in vector space.         | Captures temporal and spectral audio features.  | OpenL3, VGGish                 |\n",
    "| **Video Embeddings**             | Capture spatial and temporal video information. | Useful for video analysis and action recognition. | 3D CNNs, I3D                  |\n",
    "| **Graph Embeddings**             | Represent nodes and their relationships.         | Models complex network structures.              | Node2Vec, DeepWalk             |\n",
    "| **JSON Embeddings**              | Represent structured hierarchical data.          | Captures nested relationships in JSON data.     | Tree-LSTM, json2vec           |\n",
    "| **Multi-modal Embeddings**       | Integrate multiple data types (text, image, audio). | Enables cross-modal understanding and reasoning. | CLIP, LXMERT, ViLBERT, VisualBERT |\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Use Cases**\n",
    "\n",
    "- **Word Embeddings:**  \n",
    "  Best for basic semantic similarity tasks (e.g., word similarity, basic search).  \n",
    "\n",
    "- **Sentence/Document Embeddings:**  \n",
    "  Ideal for document classification, summarization, and information retrieval.  \n",
    "\n",
    "- **Contextual Embeddings:**  \n",
    "  Best for sentiment analysis, question-answering, and tasks needing deep contextual understanding.  \n",
    "\n",
    "- **Specialized Embeddings:**  \n",
    "  Useful for handling rare words, multilingual content, and domain-specific analysis.  \n",
    "\n",
    "- **Image Embeddings:**  \n",
    "  Perfect for image classification, object detection, and pattern recognition tasks.  \n",
    "\n",
    "- **Audio Embeddings:**  \n",
    "  Essential for audio event detection, music genre classification, and speech recognition.  \n",
    "\n",
    "- **Video Embeddings:**  \n",
    "  Applied in video action recognition, surveillance, and sports analytics.  \n",
    "\n",
    "- **Graph Embeddings:**  \n",
    "  Useful for social network analysis, fraud detection, and recommendation systems.  \n",
    "\n",
    "- **JSON Embeddings:**  \n",
    "  Suitable for tasks involving hierarchical data structures like semantic parsing.  \n",
    "\n",
    "- **Multi-modal Embeddings:**  \n",
    "  Critical for combining text, image, audio, and video data in applications like AI assistants, autonomous driving, and multimedia search engines.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
